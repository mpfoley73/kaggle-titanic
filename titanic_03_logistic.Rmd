---
title: "Kaggle - Titanic"
subtitle: "Step 3: Logistic Regression"
author: "Michael Foley"
date: "5/19/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The logistic regression model is simple and can be used for inference.  The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The model is fit with maximum likelihood estimation.  There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. 

The logistic model run with the full set of predictors had a holdout set accuracy of 0.8136, sensitivity of 0.6618, specificity of 0.9083, and AUC of 0.8660.


# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(plotROC)
library(precrec)
```

The initial data management created the data set `full`, with training rows indexed by `train_index`.  I added another predictor variable in the exploratory analysis.  I'll split this into `training` and `testing`, then 80:20 split `training` into `training_80` for training and `training_20` to compare models.

```{r warning=FALSE, message=FALSE}
load("./titanic_02.RData")

training <- full[train_index, ]
testing <- full[-train_index, ]
 
set.seed(1920)
partition <- createDataPartition(training$Survived, p = 0.80, list = FALSE)
training_80 <- training[partition, ]
training_20 <- training[-partition, ]
rm(partition)
```

I'm using 10-fold CV, but it may be interesting to [compare with bootstrapping](http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0) and other strategies. 

```{r}
train_control <- trainControl(
#  method = "boot", number = 25,
  method = "cv", number = 10,
  savePredictions = "final",
  classProbs = TRUE,                 # return predicted classes AND probabilities
  summaryFunction = twoClassSummary  # calculate sensitivity, specificity, and AUC
)
```

I'll try two models: a "full" model with (almost) all predictors, and a parsimonious model using `glmStepAIC`.  

# Model

By "all predictors", I mean all non-character predictors - I'm throwing out `Surname`, `Name`, `Ticket`, and `Cabin`.  I'm also throwing out `Fare` since it is redundant to `FarePerPass`, and `TicketN` since I binned it into `TktSize`.  That leaves me with `PassengerID`, `Survived`, and 14 predictors.

```{r}
mdl_vars <- subset(colnames(training_80), !colnames(training_80) %in% 
                     c("Surname", "Name", "Ticket", "TicketN", "Cabin", "Fare"))
mdl_vars
```

I'll use the recipe method to train.  From the exploratory analysis section I've decided to create interactions `FamSize:Sex`, `NetSurv:Sex`, and `Age*AgeCohort*Sex`.  My formula also removes dummy variable `Title_Miss` to avoid rank deficiency - I think `Title` and `Sex` are somewhat redundant.  I'm also adding some other interaction terms

```{r}
rcpe <- recipe(Survived ~ ., data = training_80[, mdl_vars]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~
                  starts_with("Sex_"):starts_with("FamSize_") +
                  starts_with("Sex_"):starts_with("Pclass_") +
                  starts_with("Sex_"):starts_with("Title_") +
                  starts_with("Sex_"):starts_with("SibSp_") +
                  starts_with("Sex_"):starts_with("Parch_") +
                  starts_with("Sex_"):starts_with("Deck_") +
                  starts_with("Sex_"):starts_with("TicketSize_") +
                  starts_with("Sex_"):NetSurv +
                  Age:starts_with("AgeCohort_"):starts_with("Sex_")) %>%
  step_rm(Title_Miss)  # to avoid rank deficiency

prep(rcpe, training = training_80)
```

# Full Model

The logistic model run with the full set of predictors had a holdout set accuracy of 0.8136, sensitivity of 0.6618, specificity of 0.9083, and AUC of 0.8660.

```{r}
set.seed(1970)
mdl_full <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_full)
```
Wow, a lot of parameters fail the significance test: `Age`, `FairPerPass`, `Embarked`, `Deck`, `FamSize`, `TktSize`, and my interaction variables.

## Resampling Performance

The model performance on the 10-fold resampling was:

```{r}
mdl_full
```
The sensitivity is 0.893 and the specificity is 0.722, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.887.  The accuracy from the confusion matrix below is 0.828.

```{r}
confusionMatrix(mdl_full)
```

## Holdout Performance

The model performance on the holdout data set was:

```{r}
predicted_classes <- predict(mdl_full, newdata = training_20) 
predicted_probs <- predict(mdl_full, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```
The sensitivity is 0.6618 and the specificity is 0.908, so the model is prone to *under-predicting* survivors.  The accuracy from the confusion matrix is 0.814.  `precrec::evalmod()` will calculate the confusion matrix values from the model using the holdout data set.  The AUC on the holdout set is 0.866.  [`pRoc::plot.roc()` and `plotROC::geom_roc()`](https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret) are options, but I like the way `yardstick::roc_curve()` looks.

```{r}
mdl_full_preds <- predict(mdl_full, newdata = training_20, type = "prob")
(mdl_full_eval <- evalmod(
  scores = mdl_full_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_full_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Full Model ROC Curve, Test Data",
    subtitle = "AUC = 0.866"
  )
```

The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.

274 of the 714 passengers survived.

* The gain curve encountered 137 survivors (50%) within the first 146 observations (20%). 

* It encountered all 274 survivors on the 714th observation.

* The bottom of the gray area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 137/714=19% of the observations.

```{r}
data.frame(
  pred = mdl_full_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Full Model Gain Curve on Holdout Set"
  )
```


# glmStepAIC

```{r}
set.seed(1970)
mdl_step <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glmStepAIC",
  family = "binomial",
  trace = FALSE,  # suppress the iteration sumaries
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_step)
```
Most of the non-significant estimators from the full model are missing here.  Exceptions are `Embarked` and `Deck`.

```{r}
mdl_step
```
The sensitivity is 0.889 and the specificity is 0.748, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.890.  Here is a graph of the ROC curve.

```{r}
options(yardstick.event_first = FALSE)  # set the second level as success
mdl_step$pred %>%
  yardstick::roc_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "glmStepAIC ROC Curve, Training Data",
    subtitle = "AUC = 0.887"
  )
```

Here are the confusion matrix values from the model using the holdout data set.

```{r}
scores <- predict(mdl_step, newdata = training_20, type = "prob")
evalmod(
  scores = predict(mdl_step, newdata = training_20, type = "prob")$Yes,
  labels = training_20$Survived
)

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  Yes = predict(mdl_step, newdata = training_20, type = "response")$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "glmSTEPAIC ROC Curve, Holdout Data",
    subtitle = "AUC = 0.866"
  )
```

The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. 

* The gain curve encountered 137 survivors (50%) within the first 145 observations (20%). 

* It encountered all 274 survivors on the 714th observation.

```{r}
options(yardstick.event_first = FALSE)  # set the second level as success
mdl_step$pred %>%
  yardstick::gain_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "glmStepAIC Gain Curve"
  )
```

Here's a more detailed look at the accuracy/specificity calculations, first with the training set,

```{r}
confusionMatrix(mdl_step)
```
And next with the hold-out test set.

```{r}
predicted_classes <- predict(mdl_step, newdata = training_20) 
predicted_probs <- predict(mdl_step, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived)
```
The accuracy on the test set is 81.92%.

# Conclusions

Compare the models 
```{r}
scores_list <- join_scores(
  predict(mdl_full, newdata = training_20, type = "prob")$Yes,
  predict(mdl_step, newdata = training_20, type = "prob")$Yes
)
labels_list <- join_labels(
  training_20$Survived,
  training_20$Survived
)

pe <- evalmod(
  scores = scores_list, 
  labels = labels_list,
  modnames = c("Full", "glmStepAIC"),
  posclass = "Yes")

autoplot(pe, "ROC")
```

```{r}
pe
```

```{r}
resamps <- resamples(list('Full' = mdl_full, 'Reduced' = mdl_step))
summary(resamps)
bwplot(resamps, layout = c(3, 1))
```

# Save Work

```{r}
scores <- predict(mdl_full, newdata = testing) %>% {ifelse(. == "Yes", 1, 0)}
submission_file <- data.frame(PassengerId = testing$PassengerId, Survived = scores)
write.csv(submission_file, file = "submission files/titanic_03_glm.csv", row.names = FALSE)
```

