---
title: "Kaggle - Titanic"
subtitle: "Step 2: Exploratory Analysis"
author: "Michael Foley"
date: "5/7/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This is a survey of the Titanic data.  The univariate analysis characterizes the predictor distributions.  The bivariate analysis characterizes relationships among the predictors.  The influential outliers analysis searches for problematic observations.

# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)  # for nearZeroVar()
library(e1071)  # for skewness()
library(broom)  # for tidy()
library(agricolae)  # for HSD.test()
library(corrplot)
library(GGally)
library(gridExtra)
```


# Load Data

The initial data management created the data set `full`, with training rows indexed by `train_index`.

```{r warning=FALSE, message=FALSE}
load("./titanic_01.RData")

glimpse(full)
```

# Univariate Analysis

In this section I will look at data distributions (univariate analysis). For factor variables, I am interested in which have near-zero-variance. For quantitative variables, I am looking for significant skew.

```{r}
preds <- full %>% select(-Survived, -PassengerId) %>% colnames()
preds_class <- full[, preds] %>% map(class) %>% unlist()
preds_factor <- subset(preds_class, preds_class == "factor") %>% names()
preds_numeric <- subset(preds_class, preds_class == "numeric") %>% names()
rm(preds_class)

assertthat::are_equal(length(c(preds_factor, preds_numeric)), ncol(full) - 2)
```

## Factor Variables

I will inspect each factor variable, looking for near-zero-variance predictors. I won’t remove them here, but I might remove them in the modeling pre-process step if they cause zero-variance in cv folds (see discussion here). `nearZeroVar()` defines near-zero variance as a frequency ratio (ratio of the most common value frequency to the second most common value frequency) >= 19 and a percent of unique values <= 10%. (DataCamp course Machine Learning Toolbox suggests more aggressive thresholds of frequency ratio >= 2 and percent of unique values <= 20%).

```{r warning=FALSE}
dummies <- dummyVars(~., data = full)
full_dummy <- as.data.frame(predict(dummies, full))
(nzv <- full_dummy %>%
    select(-PassengerId, -Survived.0, -Survived.1) %>%
    nearZeroVar(saveMetrics= TRUE)
)

nzv %>%
  data.frame() %>% rownames_to_column(var = "col") %>%
  separate(col, sep = "\\.", into = c("col", "level"), ) %>%
  filter(!is.na(level)) %>%
  ggplot(aes(x = freqRatio, y = percentUnique, 
             color = fct_rev(factor(nzv, labels = c("(okay)", "NZV"))), 
             label = level)) +
  geom_text(check_overlap = TRUE, size = 2, na.rm = TRUE) +
  geom_point(size = 3, alpha = 0.6, na.rm = TRUE) +
  geom_hline(yintercept = 10, linetype = "dashed") +
  geom_vline(xintercept = 95/5, linetype = "dashed") +
  theme(legend.position = "top") +
  labs(title = "Near-Zero Variance of Factor Variables", color = "") +
  facet_wrap(~col)
```

Ideally, you want the vars to land in the top left quadrant. The only problematic predictor is `Title = "Master"`.  

## Quantitative Variables

Skew can contribute to violation of linearity in linear regressions. I’ll check which variables have significant skew. Skew between 0.5 and 1.0 is generally considered moderate, and skew greater than 1 severe. In the following charts, the moderately skewed predictors are colored gold and the severely skewed predictors are colored red.

```{r message=FALSE}
col_skew <- map(full[, preds_numeric], skewness) %>% unlist()
col_skew_is_mod <- names(col_skew[abs(col_skew) > .5 & abs(col_skew) <= 1.0])
col_skew_is_high <- names(col_skew[abs(col_skew) > 1.0])
p <- map(
  colnames(full[, preds_numeric]),
  ~ ggplot(full, aes_string(x = .x)) +
    geom_histogram(fill = case_when(.x %in% col_skew_is_mod ~ "goldenrod", 
                                    .x %in% col_skew_is_high ~ "orangered4",
                                    TRUE ~ "cadetblue")) +
    labs(y = "", x = "", title = .x) +
    theme(axis.text.y=element_blank(), plot.title = element_text(size = 10))
)
exec(grid.arrange, ncol = 2, !!!p)
```




# Bivariate Analysis

In this section I will look at inter-variable relationships. For factor variables, I am interested in which levels have significantly different mean SalePrice values. For quantitative variables, I am looking for linear relationships with SalePrice and low correlations with each other.
 

# Influential Outliers

Now I’ll look at influential outliers. When building a predictive model, I only need to address influential outliers for variables that are likely to be important in the final model. I’ll identify the most likely important variables with a correlation matrix of the numerical predictors. For every variable with correlation >= 0.5 (a moderate correlation), I’ll check the variable distribution to see whether any influential outliers exist.

```{r}
predictors <- full %>% select(-PassengerId) %>% colnames()
frm <- formula(Survived ~ .)

```


```{r cache=TRUE}
dummies <- dummyVars(~., data = full)
full_dummy <- as.data.frame(predict(dummies, full))


mdl_mtrx <- model.matrix(frm, full[train_index, ])
cor_out <- mdl_mtrx %>% data.frame() %>%
  mutate(Survived = as.numeric(full[train_index, ]$Survived)) %>%
  cor()
cor_out_2 <- cor_out[, "Survived"] %>% abs() %>% sort(decreasing = TRUE)
cor_out_2[2:6]
```

```{r message=FALSE}
full %>% 
  ggpairs(mapping = aes(col = Survived, alpha = 0.3), 
                    lower = list(combo = wrap("facethist", bins = 20)))
```

One last look at the decks.  Here are the survival rates.

```{r}
mosaicplot(data = full[train_index,], Deck ~ Survived, color = TRUE, main = NULL)
```

Not much difference between decks B-D.







# Save Work

```{r}
#save(d_raw_1, file = "./titanic_02.RData")
```

