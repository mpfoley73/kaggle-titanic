---
title: "Kaggle - Titanic"
subtitle: "Step 4: Regularization"
author: "Michael Foley"
date: "6/11/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Regularization is a set of methods that manage the bias-variance trade-off problem in linear regression. 

Whereas OLS estimates the linear model coefficients by minimizing the loss function

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2$$

ridge regression minimizes the loss function

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \hat{\beta}_j^2,$$

lasso regression minimizes the loss function

$$L = \sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2 + \lambda \sum_{j=1}^k \left| \hat{\beta}_j \right|,$$

and elastic net minimizes the loss function

$$L = \frac{\sum_{i = 1}^n \left(y_i - x_i^{'} \hat\beta \right)^2}{2n} + \lambda \frac{1 - \alpha}{2}\sum_{j=1}^k \hat{\beta}_j^2 + \lambda \alpha\left| \hat{\beta}_j \right|.$$

The best model was elastic net. The model run on the full training data set had a holdout set accuracy of 0.8305.

After fitting to the full training data set, the performance on the testing data set on kaggle was 0.78468 accuracy - a half percent worse than the straight logistic regression 0.78947 accuracy.


# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(plotROC)
library(precrec)
```

The initial data management created the data set `full`, with training rows indexed by `train_index`.  I added another predictor variables in the exploratory analysis and split the data into `training` and `testing`, then 80:20 split `training` into `training_80` for training and `training_20` to compare models.

```{r warning=FALSE, message=FALSE}
load("./titanic_02.RData")
```

I'll use 10-fold CV. 

```{r}
train_control <- trainControl(
  method = "cv", number = 10,
  savePredictions = "final",
  classProbs = TRUE
)
```

I'll try three models: ridge, lasso, and elastic net.  

# Model

My model data set variables are `PassengerID`, `Survived`, and 13 predictors.

```{r}
mdl_vars <- c("PassengerId", mdl_vars)
mdl_vars
```

I'll use the recipe method to train.  From the exploratory analysis section I've decided to create interactions `Pclass*Sex`, `Embarked*Sex`, `TicketN:TicketNCohort`, and `Age*AgeCohort*Sex`. My recipe does *not* center and scale the predictors because the `glmnet()` algorithm will do it automatically.

```{r}
rcpe <- recipe(Survived ~ ., data = training_80[, mdl_vars]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  # model will center/scale - don't do it here
  # step_center(Age, SibSp, Parch, TicketN, FarePerPass, NetSurv) %>%
  # step_scale(Age, SibSp, Parch, TicketN, FarePerPass, NetSurv) %>%
  step_interact(terms = ~
                  starts_with("Sex_"):starts_with("Pclass_") +
                  starts_with("Sex_"):starts_with("Embarked_") +
                  TicketN:starts_with("TicketNCohort_") +
                  Age:starts_with("AgeCohort_"):starts_with("Sex_")) 

prep(rcpe, training = training_80)
```

# Ridge

The ridge model had a holdout set accuracy of 0.8249, sensitivity of 0.6324, specificity of 0.9450, and AUC of 0.8768.

Fit the ridge model by specifying tuning parameter `alpha = 0` (meaning percentage mix between ridge and lasso as 0% lasso). I set up the tuning grid for `lambda` with a little trial and error. Here is the fit summary.

```{r message=FALSE}
set.seed(1970)
mdl_ridge <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glmnet",
  family = "binomial",
  tuneGrid = expand.grid(
    .alpha = 0,  # optimize a ridge regression
    .lambda = seq(0, 2, length.out = 101)
  ),
  trControl = train_control,
  metric = "Accuracy"
)
mdl_ridge$bestTune
```
The best tuning parameter is `lambda = 0.04`, so almost no penalty on the coefficient estimator sizes. You can see the relationship between accuracy and `lambda` in the plot.

```{r}
ggplot(mdl_ridge) +
  labs(title = "Ridge Regression Parameter Tuning", x = "lambda")
```

`varImp()` ranks the predictors by the absolute value of the coefficients in the tuned model. The most important variable here was `AgeCohort`. In the straight logistic regression model, it was `NetSurv`, followed by `Pclass`.

```{r}
varImp(mdl_ridge)
```

## Resampling Performance

The accuracy from the confusion matrix is 0.8389.

```{r}
confusionMatrix(mdl_ridge)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_ridge, newdata = training_20) 
predicted_probs <- predict(mdl_ridge, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```
The sensitivity is 0.6324 and the specificity is 0.9450, so the model is prone to *under-predicting* survivors.  The accuracy from the confusion matrix is 0.8249.  `precrec::evalmod()` will calculate the confusion matrix values from the model using the holdout data set.  The AUC on the holdout set is 0.8768.

```{r}
mdl_ridge_preds <- predict(mdl_ridge, newdata = training_20, type = "prob")
(mdl_ridge_eval <- evalmod(
  scores = mdl_ridge_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_ridge_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Ridge Model ROC Curve, Test Data",
    subtitle = "AUC = 0.8783"
  )
```

The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 37 observations (21%). 

* It encountered the 68th survivor on the 163th observation (92%).

* The bottom of the gray area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 68/177=38% of the observations.

```{r}
data.frame(
  pred = mdl_ridge_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Ridge Model Gain Curve on Holdout Set"
  )
```


# Lasso

The lasso model had a holdout set accuracy of 0.8305 - a little better than ridge's 0.8249, sensitivity of 0.6618, specificity of 0.9358, and AUC of 0.8651.

Fit the lasso model by specifying tuning parameter `alpha = 1` (meaning percentage mix between ridge and lasso as 100% lasso). I set up the tuning grid for `lambda` with a little trial and error. Here is the fit summary.

```{r}
set.seed(1970)
mdl_lasso <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glmnet",
  family = "binomial",
  tuneGrid = expand.grid(
    .alpha = 1,  # optimize a lasso regression
    .lambda = seq(0, 1, length.out = 101)
  ),
  trControl = train_control,
  metric = "Accuracy"
)
mdl_lasso$bestTune
```
The best tuning parameter is `lambda = 0.02`, so again almost no penalty on the coefficient estimator sizes. You can see the relationship between accuracy and `lambda` in the plot.

```{r}
ggplot(mdl_lasso) +
  labs(title = "Lasso Regression Parameter Tuning", x = "lambda")
```

The most important variable was `Sex`, then relative importance drops quickly.  Second place goes to `NetSurv`. In ridge, second place was `AgeCohort`, then `Sex`. Lasso looks more like the straight logistic regression model.

```{r}
varImp(mdl_lasso)
```

## Resampling Performance

The accuracy from the confusion matrix is 0.8375 - a little worse than ridge's 0.8389.

```{r}
confusionMatrix(mdl_lasso)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_lasso, newdata = training_20) 
predicted_probs <- predict(mdl_lasso, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```

The sensitivity is 0.6618 and the specificity is 0.9358.  The accuracy is 0.8305 - a little better than ridge's 0.8249. The AUC on the holdout set is 0.8651.

```{r}
mdl_lasso_preds <- predict(mdl_lasso, newdata = training_20, type = "prob")
(mdl_lasso_eval <- evalmod(
  scores = mdl_lasso_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_lasso_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Lasso Model ROC Curve, Test Data",
    subtitle = "AUC = 0.8522"
  )
```

Here is the gain curve. 

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 37 observations (21%). 

* It encountered the 68th survivor on the 159th observation (89%).

```{r}
data.frame(
  pred = mdl_lasso_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Lasso Model Gain Curve on Holdout Set"
  )
```

# Elastic Net

The elastic net model had a holdout set accuracy of 0.8249 - same as ridge, sensitivity of 0.6324, specificity of 0.9450, and AUC of 0.8783.

Fit the elastic net model by tuning both `lambda` and `alpha`. I set up the tuning grid with by trial and error. Here is the fit summary.

```{r}
set.seed(1970)
mdl_elnet <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glmnet",
  family = "binomial",
  tuneGrid = expand.grid(
    .alpha = seq(0, 1, length.out = 11),  # optimize an elastic net regression
    .lambda = seq(0, 1, length.out = 101)
  ),
  trControl = train_control,
  metric = "Accuracy"
)
mdl_elnet$bestTune
```
The best tuning parameter is `alpha = 0` and `lambda = 0.03`, so no mixing at all (100% ridge) and almost no penalty on the coefficient estimator sizes.

```{r}
ggplot(mdl_elnet) +
  labs(title = "Elastic Net Regression Parameter Tuning", x = "lambda")
```

This might be a better way of looking at it. The AUC is maximized with very little regularization. As regularization increases, the ridge model performs relatively well compared to lasso.

```{r}
glmnPlot <- plot(mdl_elnet,
                 plotType = "level",
                 cuts = 15,
                 scales = list(x = list(rot = 90, cex = .65)))
update(glmnPlot,
       ylab = "Mixing Percentage\nRidge <---------> Lasso",
       sub = "",
       main = "Area Under the ROC Curve",
       xlab = "Amount of Regularization")
```

The most important variable was `AgeCohort` again.

```{r}
varImp(mdl_elnet)
```

## Resampling Performance

The accuracy from the confusion matrix is 0.8403 - a little better than ridge.

```{r}
confusionMatrix(mdl_elnet)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_elnet, newdata = training_20) 
predicted_probs <- predict(mdl_elnet, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```

The sensitivity is 0.6324 and the specificity is 0.9450.  The accuracy is 0.8249 - same as ridge. The AUC on the holdout set is 0.8783.

```{r}
mdl_elnet_preds <- predict(mdl_elnet, newdata = training_20, type = "prob")
(mdl_elnet_eval <- evalmod(
  scores = mdl_elnet_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_elnet_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Elastic Net Model ROC Curve, Test Data",
    subtitle = "AUC = 0.8733"
  )
```

Here is the gain curve. 

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 37 observations (21%). 

* It encountered the 68th survivor on the 163th observation (92%).

```{r}
data.frame(
  pred = mdl_elnet_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Elastic Net Model Gain Curve on Holdout Set"
  )
```


# Conclusions

Compare the models with `evalmod()`.

```{r}
scores_list <- join_scores(
  predict(mdl_ridge, newdata = training_20, type = "prob")$Yes,
  predict(mdl_lasso, newdata = training_20, type = "prob")$Yes,
  predict(mdl_elnet, newdata = training_20, type = "prob")$Yes
)
labels_list <- join_labels(
  training_20$Survived,
  training_20$Survived,
  training_20$Survived
)

pe <- evalmod(
  scores = scores_list, 
  labels = labels_list,
  modnames = c("Ridge", "Lasso", "Elastic Net"),
  posclass = "Yes")

autoplot(pe, "ROC")
```

```{r}
pe
```
The highest AUC was with elastic net. Elastic net also had the bestmedian accuracy.

```{r}
resamps <- resamples(list('Ridge' = mdl_ridge, 
                          'Lasso' = mdl_lasso,
                          'Elastic Net' = mdl_elnet))
summary(resamps)
bwplot(resamps, layout = c(3, 1))
```

# Refit Final Model

I'll do a final fit with the elastic net model to the entire `training` set to predict on `testing`.

Here is the fit summary.

```{r}
set.seed(1970)
mdl_final <- train(
  rcpe,
  training[, mdl_vars],
  method = "glmnet",
  family = "binomial",
  tuneGrid = expand.grid(
    .alpha = 0.0,
    .lambda = 0.03
  ),
  trControl = train_control,
  metric = "Accuracy"
)
mdl_final$bestTune
```
`AgeCohort` continues to be the most important predictor.

```{r}
varImp(mdl_final)
```

## Resampling Performance

The accuracy from the confusion matrix below is 0.8305.

```{r}
confusionMatrix(mdl_final)
```

## Create Submission File

```{r}
preds <- predict(mdl_final, newdata = testing) %>% {ifelse(. == "Yes", 1, 0)}
sub_file <- data.frame(PassengerId = testing$PassengerId, Survived = preds)
write.csv(sub_file, file = "./titanic_04_glmnet.csv", row.names = FALSE)
```


