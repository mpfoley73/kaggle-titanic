---
title: "Kaggle - Titanic"
subtitle: "Step 3: Logistic Regression"
author: "Michael Foley"
date: "5/19/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The logistic regression model is simple and can be used for inference.  The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The model is fit with maximum likelihood estimation.  There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. 

The logistic model run with the final set of predictors had a holdout set accuracy of 0.8305, sensitivity of 0.6029, specificity of 0.9725, and AUC of 0.8821.

After fitting to the full training data set, the performance on the testing data set on kaggle was 0.78947 accuracy.


# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(plotROC)
library(precrec)
```

The initial data management created the data set `full`, with training rows indexed by `train_index`.  I added another predictor variable in the exploratory analysis.  I'll split this into `training` and `testing`, then 80:20 split `training` into `training_80` for training and `training_20` to compare models.

```{r warning=FALSE, message=FALSE}
load("./titanic_02.RData")

training <- full[train_index, ]
testing <- full[-train_index, ]
 
set.seed(1920)
partition <- createDataPartition(training$Survived, p = 0.80, list = FALSE)
training_80 <- training[partition, ]
training_20 <- training[-partition, ]
rm(partition)
```

I'm using 10-fold CV, but it may be interesting to [compare with bootstrapping](http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0) and other strategies. 

```{r}
train_control <- trainControl(
#  method = "boot", number = 25,
  method = "cv", number = 10,
  savePredictions = "final",
  classProbs = TRUE,                 # return predicted classes AND probabilities
  summaryFunction = twoClassSummary  # calculate sensitivity, specificity, and AUC
)
```

I'll try two models: a "full" model with (almost) all predictors, and a parsimonious model using `glmStepAIC`.  

# Model

By "all predictors", I mean all non-character predictors - I'm throwing out `Surname`, `Name`, `Ticket`, and `Cabin`.  I'm also throwing out `Fare` since it is redundant to `FarePerPass`, and `TicketN` since I binned it into `TktSize`.  That leaves me with `PassengerID`, `Survived`, and 13 predictors.

```{r}
mdl_vars <- c("PassengerId", mdl_vars)
mdl_vars
```

I'll use the recipe method to train.  From the exploratory analysis section I've decided to create interactions `FamSize:Sex`, `NetSurv:Sex`, and `Age*AgeCohort*Sex`.  My formula also removes dummy variable `Title_Miss` to avoid rank deficiency - I think `Title` and `Sex` are somewhat redundant.  I'm also adding some other interaction terms

```{r}
rcpe <- recipe(Survived ~ ., data = training_80[, mdl_vars]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~
                  starts_with("Sex_"):starts_with("Pclass_") +
                  starts_with("Sex_"):starts_with("Embarked_") +
                  TicketN:starts_with("TicketNCohort_") +
                  Age:starts_with("AgeCohort_"):starts_with("Sex_")) 

prep(rcpe, training = training_80)
```

# Full Model

The logistic model run with the full set of predictors had a holdout set accuracy of 0.8192, sensitivity of 0.6029, specificity of 0.9541, and AUC of 0.8821.

Here is the fit summary.

```{r}
set.seed(1970)
mdl_full <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_full)
```
`Parch`, `FarePerPass`, `Embarked`, and `Employee` fail the significance test.  `varImp()` ranks the coefficients by the absolute value of the *t*-statistic.  `NetSurv` is most important.

```{r}
varImp(mdl_full)
```

## Resampling Performance

The model performance on the 10-fold resampling was:

```{r}
mdl_full
```
The sensitivity is 0.918 and the specificity is 0.701, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.885.  The accuracy from the confusion matrix below is 0.8347.

```{r}
confusionMatrix(mdl_full)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_full, newdata = training_20) 
predicted_probs <- predict(mdl_full, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```
The sensitivity is 0.6029 and the specificity is 0.9541, so the model is prone to *under-predicting* survivors.  The accuracy from the confusion matrix is 0.8192.  `precrec::evalmod()` will calculate the confusion matrix values from the model using the holdout data set.  The AUC on the holdout set is 0.8821.  [`pRoc::plot.roc()` and `plotROC::geom_roc()`](https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret) are options, but I like the way `yardstick::roc_curve()` looks.

```{r}
mdl_full_preds <- predict(mdl_full, newdata = training_20, type = "prob")
(mdl_full_eval <- evalmod(
  scores = mdl_full_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_full_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Full Model ROC Curve, Test Data",
    subtitle = "AUC = 0.8821"
  )
```

The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The gray area is the “gain” over a random prediction.

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 37 observations (21%). 

* It encountered all 68 survivors on the 157th observation (89%).

* The bottom of the gray area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the gray area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 68/177=38% of the observations.

```{r}
data.frame(
  pred = mdl_full_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Full Model Gain Curve on Holdout Set"
  )
```


# glmStepAIC

Let's try this again with stepwise regression to see how a more parsimonious model might perform.

```{r}
set.seed(1970)
mdl_step <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glmStepAIC",
  family = "binomial",
  trace = FALSE,  # suppress the iteration summaries
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_step)
```
`glmStepAIC` dropped `Parch`, `FarePerPass`, `Embarked`, `Employee`, most of the `Deck` dummies (although it kept `DeckC`), and the sex interaction with one level of `Pclass` and one level of `Embarked`.

## Resampling Performance

The model performance on the 10-fold resampling was:

```{r}
mdl_step
```
The sensitivity is 0.9159 and the specificity is 0.6862, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.8792.  The accuracy from the confusion matrix below is 0.8277.

```{r}
confusionMatrix(mdl_step)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_step, newdata = training_20) 
predicted_probs <- predict(mdl_step, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```
The sensitivity is 0.6029 and the specificity is 0.9633, so the model is prone to *under-predicting* survivors.  The accuracy from the confusion matrix is 0.8249 - *higher* than the full model!  `precrec::evalmod()` will calculate the confusion matrix values from the model using the holdout data set.  The AUC on the holdout set is 0.8821.

```{r}
mdl_step_preds <- predict(mdl_step, newdata = training_20, type = "prob")
(mdl_step_eval <- evalmod(
  scores = mdl_step_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_step_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "glmStepAIC Model ROC Curve, Test Data",
    subtitle = "AUC = 0.8847"
  )
```

Here is the gain curve again.

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 36 observations (20%). 

* It encountered all 68 survivors on the 150th observation (85%).

```{r}
data.frame(
  pred = mdl_step_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "glmStepAIC Model Gain Curve on Holdout Set"
  )
```


# Final Model

The stepwise regression actually performed better than the full model (surprising - can multicollinearity reduce predictive accuracy on the holdout?).  I don't want to just use the stepwise model though, because it keeps selected factor variable levels, and if I'm going to keep one level, I want the entire factor variable.  So I'll fit this one more time, just dropping the variables where `glmStepAIC` dropped the entire variable.

First I'll fit it with the `training_80` set, just so I can compare.  Then I'll do a final fit to the entire `training` set to predict on `testing`.

```{r}
mdl_vars_final <- subset(mdl_vars, !mdl_vars %in% c("Parch", "FarePerPass", "Employee"))
rcpe_final <- recipe(Survived ~ ., data = training_80[, mdl_vars_final]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~
                  starts_with("Sex_"):starts_with("Pclass_") +
                  starts_with("Sex_"):starts_with("Embarked_") +
                  TicketN:starts_with("TicketNCohort_") +
                  Age:starts_with("AgeCohort_"):starts_with("Sex_")) 

prep(rcpe_final, training = training_80)
```

The logistic model run with the final set of predictors had a holdout set accuracy of 0.8305, sensitivity of 0.6029, specificity of 0.9725, and AUC of 0.8821.

Here is the fit summary.

```{r}
set.seed(1970)
mdl_final_train <- train(
  rcpe_final,
  training_80[, mdl_vars_final],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_final_train)
```
`NetSurv` continues to be the most important predictor.

```{r}
varImp(mdl_final_train)
```

## Resampling Performance

The model performance on the 10-fold resampling was:

```{r}
mdl_final_train
```
The sensitivity is 0.9250 and the specificity is 0.7007, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.8856.  The accuracy from the confusion matrix below is 0.8389.

```{r}
confusionMatrix(mdl_final_train)
```

## Holdout Performance

Here is the model performance on the holdout data set.

```{r}
predicted_classes <- predict(mdl_final_train, newdata = training_20) 
predicted_probs <- predict(mdl_final_train, newdata = training_20, type = "prob")
confusionMatrix(predicted_classes, training_20$Survived, positive = "Yes")
```
The sensitivity is 0.6029 and the specificity is 0.9725, so the model is prone to *under-predicting* survivors.  The accuracy from the confusion matrix is 0.8305. The AUC on the holdout set is 0.8821.

```{r}
mdl_final_train_preds <- predict(mdl_final_train, newdata = training_20, type = "prob")
(mdl_final_train_eval <- evalmod(
  scores = mdl_final_train_preds$Yes,
  labels = training_20$Survived
))

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  pred = mdl_final_train_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Final Model (Training) ROC Curve, Test Data",
    subtitle = "AUC = 0.8774"
  )
```

68 of the 177 passengers in the holdout set survived.

* The gain curve encountered 34 survivors (50%) within the first 37 observations (21%). 

* It encountered all 68 survivors on the 156th observation (88%).

```{r}
data.frame(
  pred = mdl_final_train_preds$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::gain_curve(obs, pred) %>%
  autoplot() +
  labs(
    title = "Final Model (Training) Gain Curve on Holdout Set"
  )
```

# Conclusions

Compare the models with `evalmod()`.

```{r}
scores_list <- join_scores(
  predict(mdl_full, newdata = training_20, type = "prob")$Yes,
  predict(mdl_step, newdata = training_20, type = "prob")$Yes,
  predict(mdl_final_train, newdata = training_20, type = "prob")$Yes
)
labels_list <- join_labels(
  training_20$Survived,
  training_20$Survived,
  training_20$Survived
)

pe <- evalmod(
  scores = scores_list, 
  labels = labels_list,
  modnames = c("Full", "glmStepAIC", "Final (Training)"),
  posclass = "Yes")

autoplot(pe, "ROC")
```

```{r}
pe
```
The highest AUC was with the first, full model.

```{r}
resamps <- resamples(list('Full' = mdl_full, 
                          'Reduced' = mdl_step,
                          'Final' = mdl_final_train))
summary(resamps)
bwplot(resamps, layout = c(3, 1))
```

# Refit Final Model

I'll do a final fit to the entire `training` set to predict on `testing`.

Here is the fit summary.

```{r}
set.seed(1970)
mdl_final <- train(
  rcpe_final,
  training[, mdl_vars_final],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_final)
```
`NetSurv` continues to be the most important predictor.

```{r}
varImp(mdl_final)
```

## Resampling Performance

The model performance on the 10-fold resampling was:

```{r}
mdl_final
```
The sensitivity is 0.9034 and the specificity is 0.6989, so the model is prone to *over-predicting* survivors.  The area under the ROC curve is 0.8876.  The accuracy from the confusion matrix below is 0.8249.

```{r}
confusionMatrix(mdl_final)
```

## Create Submission File

```{r}
preds <- predict(mdl_final, newdata = testing) %>% {ifelse(. == "Yes", 1, 0)}
sub_file <- data.frame(PassengerId = testing$PassengerId, Survived = preds)
write.csv(sub_file, file = "./titanic_03_glm.csv", row.names = FALSE)
```


