---
title: "Kaggle - Titanic"
subtitle: "Step 2: Exploratory Analysis"
author: "Michael Foley"
date: "5/7/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


This is a survey of the Titanic data.  The univariate analysis characterizes the predictor distributions.  The bivariate analysis characterizes relationships among the predictors.  The influential outliers analysis searches for problematic observations.

# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)  # for nearZeroVar()
library(e1071)  # for skewness()
library(broom)  # for tidy()
library(agricolae)  # for HSD.test()
library(corrplot)
library(GGally)
```


# Load Data

The initial data management created the data set `full`, with training rows indexed by `train_index`.

```{r warning=FALSE, message=FALSE}
load("./titanic_01.RData")

glimpse(full)
```

# Univariate Analysis

In this section I will look at data distributions (univariate analysis). For factor variables, I am interested in which have near-zero-variance. For quantitative variables, I am looking for significant skew.

```{r}
preds_class <- full %>% select(-Survived, -PassengerId) %>% map(class) %>% unlist()
preds_factor <- subset(preds_class, preds_class == "factor") %>% names()
preds_numeric <- subset(preds_class, preds_class == "numeric") %>% names()
rm(preds_class)

assertthat::are_equal(length(c(preds_factor, preds_numeric)), ncol(full) - 2)
```

I will inspect each factor variable, looking for near-zero-variance predictors. I won’t remove them here, but I might remove them in the modeling pre-process step if they cause zero-variance in cv folds (see discussion here). caret has a nice function nearZeroVar() to identify near-zero variance predictors. It defines near-zero variance as a frequency ratio (ratio of the most common value frequency to the second most common value frequency) >= 19 and a percent of unique values <= 10% (see caret package documentation). (DataCamp course Machine Learning Toolbox suggests more aggressive thresholds of frequency ratio >= 2 and percent of unique values <= 20%).

```{r}
(nzv <- nearZeroVar(full[, preds_factor], saveMetrics= TRUE))
```


```{r}
full %>% count(Sex)
```

# Bivariate Analysis

In this section I will look at inter-variable relationships. For factor variables, I am interested in which levels have significantly different mean SalePrice values. For quantitative variables, I am looking for linear relationships with SalePrice and low correlations with each other.
 

# Influential Outliers

Now I’ll look at influential outliers. When building a predictive model, I only need to address influential outliers for variables that are likely to be important in the final model. I’ll identify the most likely important variables with a correlation matrix of the numerical predictors. For every variable with correlation >= 0.5 (a moderate correlation), I’ll check the variable distribution to see whether any influential outliers exist.

```{r}
predictors <- full %>% select(-PassengerId) %>% colnames()
frm <- formula(Survived ~ .)

```


```{r cache=TRUE}
mdl_mtrx <- model.matrix(frm, full[train_index, ])
cor_out <- mdl_mtrx %>% data.frame() %>%
  mutate(Survived = as.numeric(full[train_index, ]$Survived)) %>%
  cor()
cor_out_2 <- cor_out[, "Survived"] %>% abs() %>% sort(decreasing = TRUE)
cor_out_2[2:6]
```

```{r message=FALSE}
full %>% 
  ggpairs(mapping = aes(col = Survived, alpha = 0.3), 
                    lower = list(combo = wrap("facethist", bins = 20)))
```






# Save Work

```{r}
#save(d_raw_1, file = "./titanic_02.RData")
```

