---
title: "Kaggle - Titanic"
subtitle: "Step 3: Logistic Regression"
author: "Michael Foley"
date: "5/19/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The logistic regression model is simple and can be used for inference.  The model produced in this section had an overall accuracy of 0.8305, a sensitivity of 0.9358, specificity of 0.6618, and AUC of 0.882.

The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The model is fit with maximum likelihood estimation.  There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. 


# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(plotROC)
#library(broom)
```

The initial data management created the data set `full`, with training rows indexed by `train_index`.  I added another predictor variable in the exploratory analysis.  I'll split this into `training` and `testing`, then 80:20 split `training` into `training_80` for training and `training_20` to compare models.

```{r warning=FALSE, message=FALSE}
load("./titanic_02.RData")

training <- full[train_index, ]
testing <- full[-train_index, ]
 
set.seed(1920)
partition <- createDataPartition(training$Survived, p = 0.80, list = FALSE)
training_80 <- training[partition, ]
training_20 <- training[-partition, ]
rm(partition)
```

I'm using 10-fold CV, but it may be interesting to [compare with bootstrapping](http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0) and other strategies. 

```{r}
train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,                 # return predicted classes AND probabilites
  summaryFunction = twoClassSummary  # calculate sensitivity, specificity, and AUC
)
```

I'll try two models: a "kitchen sink" model with (almost) all predictors, and a parsimonious model.  

# Kitchen Sink Model

By "all predictors", I mean all non-character predictors - I'm throwing out `Surname`, `Name`, `Ticket`, and `Cabin`.  I'm also throwing out `Fare` since it is redundant to `FarePerPass` and `TktSize`.  That leaves me with `PassengerID`, `Survived`, and 15 predictors.

```{r}
mdl_vars <- subset(colnames(training_80), !colnames(training_80) %in% 
                     c("Surname", "Name", "Ticket", "Cabin", "Fare"))
mdl_vars
```

I'll use the recipe method to train.  From the exploratory analysis section I've decided to create interactions `FamSize:Sex`, `NetSurv:Sex`, and `Age*AgeCohort*Sex`.  My formula also removes dummy variable `Title_Miss` to avoid rank deficiency - I think `Title` and `Sex` are somewhat redundant.

```{r}
rcpe <- recipe(Survived ~ ., data = training_80[, mdl_vars]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~
                  starts_with("FamSize_"):starts_with("Sex_") +
                  NetSurv:starts_with("Sex_") +
                  Age:starts_with("Sex_")) %>%
  step_rm(Title_Miss)  # to avoid rank deficiency

prep(rcpe, training = training_80)
```

# Model Summary

```{r}
set.seed(1970)
mdl_kitch <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
summary(mdl_kitch)
mdl_kitch
```


The sensitivity is 0.895 and the specificity is 0.733, so the model is proned to *over-predicting* survivors.  The area under the ROC curve is 0.889.  Here is a graph of the ROC curve. [`pRoc::plot.roc()` and `plotROC::geom_roc()`](https://stackoverflow.com/questions/31138751/roc-curve-from-training-data-in-caret) are options, but I like the way `yardstick::roc_curve()` looks.

```{r}
options(yardstick.event_first = FALSE)  # set the second level as success
mdl_kitch$pred %>%
  yardstick::roc_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "Kitchen Sink ROC Curve, Training Data",
    subtitle = "AUC = 0.889"
  )
```

```{r}
scores <- predict(mdl_kitch, newdata = training_20, type = "prob")
evalmod(
  scores = predict(mdl_kitch, newdata = training_20, type = "prob")$Yes,
  labels = training_20$Survived
)

options(yardstick.event_first = FALSE)  # set the second level as success
data.frame(
  Yes = predict(mdl_kitch, newdata = training_20, type = "response")$Yes, 
  obs = training_20$Survived
) %>%
  yardstick::roc_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "Kitchen Sink ROC Curve, Training Data",
    subtitle = "AUC = 0.889"
  )
```


The gain curve plots the cumulative summed true outcome versus the fraction of items seen when sorted by the predicted value. The “wizard” curve is the gain curve when the data is sorted by the true outcome. If the model’s gain curve is close to the wizard curve, then the model predicted the response variable well. The grey area is the “gain” over a random prediction.

274 of the 714 passengers survived.

* The gain curve encountered 137 survivors (50%) within the first 145 observations (20%). 

* It encountered all 274 survivors on the 714th observation.

* The bottom of the grey area is the outcome of a random model. Only half the survivors would be observed within 50% of the observations. The top of the grey area is the outcome of the perfect model, the “wizard curve”. Half the survivors would be observed in 137/714=19% of the observations.

```{r}
options(yardstick.event_first = FALSE)  # set the second level as success
mdl_kitch$pred %>%
  yardstick::gain_curve(obs, Yes) %>%
  autoplot() +
  labs(
    title = "Kitchen Sink Gain Curve"
#    subtitle = "AUC = 0.889"
  )
```

Here's a more detailed look at the accuracy/specifity calculations.

```{r}
confusionMatrix(mdl_kitch)
summary(mdl_kitch)
# Predict classes and probabilities for holdout set
predicted_classes <- predict(mdl_kitch, newdata = training_20) 
predicted_probs <- predict(mdl_kitch, newdata = training_20, type = "prob")

confusionMatrix(predicted_classes, training_20$Survived)

```

```{r}
levels(training_80$Survived)
training_80 %>% count(Survived)
```

