---
title: "Kaggle - Titanic"
subtitle: "Step 3: Logistic Regression"
author: "Michael Foley"
date: "5/19/2020"
output: 
  html_document:
    theme: flatly
    toc: true
    highlight: haddock
    fig_width: 9
    fig_caption: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The logistic regression model is simple and can be used for inference.  The model produced in this section had an overall accuracy of 0.8305, a sensitivity of 0.9358, specificity of 0.6618, and AUC of 0.882.

The **binary** logistic regression model is

$$y = logit(\pi) = \ln \left( \frac{\pi}{1 - \pi} \right) = X \beta$$

where $\pi$ is the event probability. The model predicts the *log odds* of the response variable.  The model is fit with maximum likelihood estimation.  There is no closed-form solution, so GLM estimates coefficients with interatively reweighted least squares. 


# Setup

```{r message=FALSE}
library(tidyverse)
library(caret)
library(recipes)
library(broom)
```

The initial data management created the data set `full`, with training rows indexed by `train_index`.  I'll split this into `training` and `testing`, then 80:20 split `training` into `training_80` for training and `training_20` to compare models.

```{r warning=FALSE, message=FALSE}
load("./titanic_01.RData")

training <- full[train_index, ]
testing <- full[-train_index, ]
 
set.seed(1920)
partition <- createDataPartition(training$Survived, p = 0.80, list = FALSE)
training_80 <- training[partition, ]
training_20 <- training[-partition, ]
rm(partition)
```

I'm using 10-fold CV, but it may be interesting to [compare with bootstrapping](http://appliedpredictivemodeling.com/blog/2014/11/27/08ks7leh0zof45zpf5vqe56d1sahb0) and other strategies. 

```{r}
train_control <- trainControl(
  method = "cv",
  number = 10,
  savePredictions = "final",
  classProbs = TRUE,                 # return predicted classes AND probabilites
  summaryFunction = twoClassSummary  # calculate sensitivity, specificity, and AUC
)
```

I'll try two models: a kitchen sink model with all predictors, and a parsimonious model.  

# Kitchen Sink Model

By "all predictors", I mean all non-character predictors - I'm throwing out `Surname`, `Name`, `Ticket`, and `Cabin`.  I'm also throwing out `Fare` since it is redundant to `FarePerPass` and `TktSize`.  That leaves me with `PassengerID`, `Survived`, and 14 predictors.

```{r}
mdl_vars <- subset(colnames(training_80), !colnames(training_80) %in% 
                     c("Surname", "Name", "Ticket", "Cabin", "Fare"))
mdl_vars
```

I'll use the recipe method to train.  From the exploratory analysis section I've decided to create interactions `SibSp:Sex`, `Parch:Sex`, and `NetSurv:Sex`.  *Note that I remove Title

```{r}

rcpe <- recipe(Survived ~ ., data = training_80[, mdl_vars]) %>%
  update_role(PassengerId, new_role = "id variable") %>%
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_interact(terms = ~
                  SibSp:starts_with("Sex_") +
                  Parch:starts_with("Sex_") +
                  NetSurv:starts_with("Sex_")) %>%
  step_rm(Title_Miss)  # to avoid rank deficiency

prep(rcpe, training = training_80)
```

```{r}
x <- glm(Survived ~ Sex*Age, data = training_80, family = "binomial")
summary(x)

training %>% mutate(AgeBin = floor(Age / 4)*4) %>% 
  count(Sex, AgeBin, Survived) %>%
  ungroup() %>% group_by(Sex, AgeBin) %>% mutate(N = sum(n), pct = n / sum(n)) %>%
  filter(Survived == "Yes") %>%
ggplot(aes(x = as.factor(AgeBin), y = pct, fill = N)) + 
  geom_col() +
  facet_wrap(~Sex)
```

```{r}
set.seed(1970)
mdl_kitch <- train(
  rcpe,
  training_80[, mdl_vars],
  method = "glm",
  family = "binomial",
  trControl = train_control,
  metric = "ROC"
)
mdl_kitch
summary(mdl_kitch)
confusionMatrix(mdl_kitch)
# Predict classes and probabilities for holdout set
predicted_classes <- predict(mdl_kitch, newdata = training_20) 
predicted_probs <- predict(mdl_kitch, newdata = training_20, type = "prob")

confusionMatrix(predicted_classes, training_20$Survived)


```

```{r}
levels(training_80$Survived)
training_80 %>% count(Survived)
```

